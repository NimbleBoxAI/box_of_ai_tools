{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aakash/anaconda3/envs/openvino/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:970: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters:  116750336\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "import torch\n",
    "\n",
    "# Importing the tokenizer and model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "# Model parameters.\n",
    "param = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total Parameters: \", param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_ids: tensor([[ 631,  304,   27, 4185,  661,   19,   94, 2435,    4,    3]])\n",
      "Attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Creating the inputs for the model with the help of the tokenizer. \n",
    "input_dict = tokenizer(\"My name is Mariama, my favorite\", add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = input_dict.input_ids\n",
    "attn_mask = input_dict.attention_mask\n",
    "\n",
    "print(\"Input_ids:\", input_ids)\n",
    "print(\"Attention_mask\", attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[6;30;42mPyTorch output:\u001b[0m XLNetLMHeadModelOutput(loss=None, logits=tensor([[[-13.9551, -31.5801, -31.6446,  ..., -23.2029, -26.0101, -24.1034],\n",
      "         [-15.8220, -33.8814, -33.7126,  ..., -23.7666, -26.2747, -26.1308],\n",
      "         [-18.6391, -34.6777, -34.5127,  ..., -25.1410, -30.4873, -29.7563],\n",
      "         ...,\n",
      "         [-14.2356, -31.2401, -30.9187,  ..., -21.7698, -23.2759, -25.2387],\n",
      "         [-14.0066, -28.3656, -28.0234,  ..., -21.8343, -19.8477, -21.7294],\n",
      "         [-14.2283, -28.5359, -28.1960,  ..., -22.0614, -20.0291, -21.7805]]],\n",
      "       grad_fn=<AddBackward0>), mems=(tensor([[[-2.4275e-02, -5.2557e-03,  4.0634e-03,  ..., -3.6994e-02,\n",
      "          -5.4372e-02, -5.0193e-02]],\n",
      "\n",
      "        [[-1.3011e-02, -4.5154e-02, -6.1915e-02,  ...,  3.0913e-05,\n",
      "          -1.0610e-02,  7.7529e-02]],\n",
      "\n",
      "        [[-1.8461e-02, -3.5976e-02, -8.4252e-03,  ...,  2.2744e-02,\n",
      "           6.5674e-03, -2.5391e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.2049e-01, -4.0158e-02,  5.5989e-03,  ..., -1.2256e-01,\n",
      "           6.6407e-02,  7.6938e-02]],\n",
      "\n",
      "        [[ 7.8792e-02, -5.8267e-02, -9.0492e-02,  ...,  4.9333e-02,\n",
      "           6.3360e-02, -5.1997e-02]],\n",
      "\n",
      "        [[ 1.8133e-02, -1.4938e-03, -1.4942e-01,  ...,  1.1653e-03,\n",
      "          -9.3337e-04,  1.8762e-02]]]), tensor([[[-0.5093, -0.2686, -0.4775,  ..., -1.3220, -1.7983,  0.3497]],\n",
      "\n",
      "        [[-0.2590, -1.3766, -0.9275,  ..., -0.5455, -0.4462,  1.8857]],\n",
      "\n",
      "        [[-0.0968, -0.6547,  0.1543,  ..., -0.2103,  0.1944,  0.4227]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.0498, -1.0591,  0.5334,  ..., -2.6112,  1.6442,  2.2778]],\n",
      "\n",
      "        [[ 1.2250, -0.0750, -1.8214,  ...,  0.3054,  0.8025,  0.5617]],\n",
      "\n",
      "        [[ 0.4086,  0.1739, -2.5761,  ..., -0.2467, -0.0787,  1.4185]]]), tensor([[[-1.4773, -1.3516, -0.9040,  ..., -1.2361, -0.1060, -0.5139]],\n",
      "\n",
      "        [[-0.3830, -1.9161,  0.0717,  ...,  0.1043, -0.0879,  0.4449]],\n",
      "\n",
      "        [[-0.9414, -1.3701,  0.8779,  ..., -0.4689,  0.8183, -0.1124]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.2624, -0.9711,  0.7133,  ..., -1.9095,  1.5299,  1.1859]],\n",
      "\n",
      "        [[ 0.8268, -0.6267, -1.8395,  ..., -0.4600,  0.9460,  0.2834]],\n",
      "\n",
      "        [[ 0.2645, -0.5054, -2.6157,  ..., -0.4800,  0.4335,  0.8074]]]), tensor([[[-1.1712, -2.0447, -0.6295,  ..., -1.5570, -0.3596, -0.4810]],\n",
      "\n",
      "        [[-0.3175, -2.0924, -0.2256,  ..., -0.1564,  0.2791,  0.1582]],\n",
      "\n",
      "        [[-0.7426, -1.5801,  2.5681,  ..., -1.7096,  1.1835, -0.5686]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.5860, -1.1979,  0.4416,  ..., -1.8686,  2.0372, -0.0781]],\n",
      "\n",
      "        [[ 0.4182, -0.2878, -2.2078,  ..., -0.4345,  0.2792, -0.8973]],\n",
      "\n",
      "        [[ 0.1102, -0.5693, -3.2881,  ..., -0.3047, -0.1994, -0.3281]]]), tensor([[[-0.7337, -2.3569, -0.3810,  ..., -0.5875, -0.7038,  0.2475]],\n",
      "\n",
      "        [[-0.1221, -2.3088, -0.6627,  ...,  0.2985, -0.0951,  0.4293]],\n",
      "\n",
      "        [[-0.6833, -1.4297,  2.0759,  ..., -0.5291,  1.2224,  0.4722]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5433, -1.5016,  0.2298,  ..., -0.9255,  2.3122, -0.1791]],\n",
      "\n",
      "        [[ 1.1807, -0.5432, -1.3693,  ..., -0.0629, -0.4223, -0.3524]],\n",
      "\n",
      "        [[ 1.2288, -0.5631, -2.1727,  ..., -0.0845, -0.6035, -0.4353]]]), tensor([[[-0.5665, -1.5247, -0.6386,  ..., -0.9958, -0.9511,  0.3984]],\n",
      "\n",
      "        [[ 0.0139, -1.2390, -0.7164,  ..., -0.9338, -0.3926,  0.1709]],\n",
      "\n",
      "        [[-0.7822, -0.7672,  1.9044,  ..., -1.3041,  0.3638, -0.3839]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1880, -0.4743,  0.2370,  ..., -1.3739,  1.8007, -0.3842]],\n",
      "\n",
      "        [[ 0.5703,  0.0128, -0.6167,  ..., -0.1569, -0.0649, -0.1515]],\n",
      "\n",
      "        [[ 0.9855, -0.0941, -1.4672,  ..., -0.2665, -0.1883, -0.5729]]]), tensor([[[-0.4235, -1.9388, -1.2749,  ..., -1.1531, -1.1777,  1.3818]],\n",
      "\n",
      "        [[-0.3474, -1.9227, -0.4070,  ..., -0.2337, -0.9783,  1.0038]],\n",
      "\n",
      "        [[-0.9705, -0.8564,  2.6017,  ..., -0.6794,  0.0869,  0.1654]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.2066, -0.5924,  0.1913,  ..., -1.2161,  1.6523, -0.2111]],\n",
      "\n",
      "        [[ 0.6290, -0.2381, -0.7878,  ..., -0.0029, -0.5232,  0.5621]],\n",
      "\n",
      "        [[ 1.1066, -0.2928, -1.7486,  ..., -0.0903, -0.2973,  0.4494]]]), tensor([[[ 0.1953, -1.6561, -1.3397,  ..., -0.6218, -1.6523,  1.1292]],\n",
      "\n",
      "        [[-0.4875, -2.1875, -0.1207,  ..., -0.0650, -0.9179,  1.3712]],\n",
      "\n",
      "        [[-0.9846, -1.0007,  2.5615,  ..., -0.3828,  0.2153,  0.1182]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5955, -0.6031,  0.4596,  ..., -1.4512,  1.4689,  0.8745]],\n",
      "\n",
      "        [[ 0.5476, -0.1231, -0.0151,  ..., -0.0317, -0.0876,  0.3275]],\n",
      "\n",
      "        [[ 1.2520, -0.1536, -0.6876,  ..., -0.0381, -0.1308,  0.0764]]]), tensor([[[-0.2664, -1.4937, -1.0613,  ..., -1.0482, -2.1062,  0.0590]],\n",
      "\n",
      "        [[-0.1850, -1.9105, -0.3442,  ..., -0.5430, -1.5780,  0.6832]],\n",
      "\n",
      "        [[-0.7200, -0.6787,  1.9060,  ..., -1.1466, -0.5333, -0.0158]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.6145,  0.0377,  0.5307,  ..., -1.7280,  1.3491, -0.1564]],\n",
      "\n",
      "        [[ 0.2610,  0.0515, -0.1040,  ..., -0.0405, -0.1855,  0.1256]],\n",
      "\n",
      "        [[ 0.8561, -0.0553, -0.5403,  ...,  0.0314, -0.2144,  0.2120]]]), tensor([[[-2.3416e-01, -7.5437e-01, -1.3645e+00,  ..., -1.0038e+00,\n",
      "          -1.4151e+00, -1.2308e-01]],\n",
      "\n",
      "        [[-2.1868e-02, -1.2408e+00, -1.0204e+00,  ..., -3.1101e-01,\n",
      "          -1.3284e+00,  5.4349e-01]],\n",
      "\n",
      "        [[-4.9401e-01, -4.6465e-01,  1.1467e+00,  ..., -1.0392e+00,\n",
      "          -8.3904e-02,  6.4279e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4930e+00,  2.7262e-01, -1.6595e-01,  ..., -1.4887e+00,\n",
      "           1.1610e+00,  4.1068e-01]],\n",
      "\n",
      "        [[ 7.9255e-02,  5.1073e-02, -2.6073e-01,  ..., -1.9040e-01,\n",
      "           1.4168e-02,  2.6984e-01]],\n",
      "\n",
      "        [[ 2.9151e-01,  2.4979e-04, -5.2166e-01,  ..., -2.3201e-01,\n",
      "           6.5917e-02,  2.9300e-01]]]), tensor([[[-0.8031, -0.6398, -1.7356,  ..., -1.5182, -1.0163,  0.2466]],\n",
      "\n",
      "        [[-0.1965, -1.1699, -1.3517,  ..., -0.4968, -1.3780,  1.2587]],\n",
      "\n",
      "        [[-0.7260,  0.1226,  1.3041,  ..., -1.6858,  0.5961,  0.8541]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0579,  0.7730, -0.6300,  ..., -2.3414,  1.3544,  0.4055]],\n",
      "\n",
      "        [[-0.0145,  0.0320, -0.1072,  ..., -0.0054,  0.0664,  0.0779]],\n",
      "\n",
      "        [[ 0.0131,  0.0653, -0.1620,  ..., -0.0771,  0.0923,  0.0765]]]), tensor([[[-0.5650, -0.8642, -1.1652,  ..., -0.8505, -0.6685,  0.2630]],\n",
      "\n",
      "        [[-0.1747, -1.4552, -1.1054,  ...,  0.2017, -0.7389,  1.0495]],\n",
      "\n",
      "        [[-0.2598, -0.0519,  0.3461,  ..., -1.1220,  0.3509,  1.1125]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.9466, -0.0600, -0.4932,  ..., -1.3288,  1.0229,  0.3163]],\n",
      "\n",
      "        [[-0.1234,  0.1006, -0.2516,  ...,  0.0080, -0.0451,  0.4247]],\n",
      "\n",
      "        [[-0.1180,  0.1409, -0.3015,  ..., -0.0167, -0.0523,  0.4263]]])), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Storing the pytorch model's output in pt_outputs variable.\n",
    "pt_outputs = model(input_ids, attn_mask)\n",
    "\n",
    "print('\\x1b[6;30;42m' + 'PyTorch output:' + '\\x1b[0m', pt_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aakash/anaconda3/envs/openvino/lib/python3.7/site-packages/transformers/models/xlnet/modeling_xlnet.py:1173: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  non_tgt_mask = -torch.eye(qlen).to(attn_mask)\n",
      "/home/aakash/anaconda3/envs/openvino/lib/python3.7/site-packages/transformers/modeling_utils.py:1760: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n"
     ]
    }
   ],
   "source": [
    "# Exporting from torch to ONNX.\n",
    "torch.onnx.export(model,\n",
    "                 (input_ids, attn_mask),\n",
    "                 \"xlnet-base-cased.onnx\",\n",
    "                 input_names=[\"input_ids\", \"attn_mask\"],\n",
    "                 output_names=[\"outputs\"],\n",
    "                 opset_version=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/aakash/box_of_ai_tools/Natural_Language_Processing/Senitment_Analysis/Xlnet base cased/xlnet-base-cased.onnx\n",
      "\t- Path for generated IR: \t/home/aakash/box_of_ai_tools/Natural_Language_Processing/Senitment_Analysis/Xlnet base cased/.\n",
      "\t- IR output name: \txlnet-base-cased\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "ONNX specific parameters:\n",
      "Model Optimizer version: \t2021.2.0-1877-176bdf51370-releases/2021/2\n",
      "[ WARNING ]  Convert data type of Parameter \"input_ids\" to int32\n",
      "[ WARNING ]  Convert data type of Parameter \"attn_mask\" to int32\n",
      "[ ERROR ]  Cannot infer shapes or values for node \"Einsum_56\".\n",
      "[ ERROR ]  There is no registered \"infer\" function for node \"Einsum_56\" with op = \"Einsum\". Please implement this function in the extensions. \n",
      " For more information please refer to Model Optimizer FAQ, question #37. (https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html?question=37#question-37)\n",
      "[ ERROR ]  \n",
      "[ ERROR ]  It can happen due to bug in custom shape infer function <UNKNOWN>.\n",
      "[ ERROR ]  Or because the node inputs have incorrect values/shapes.\n",
      "[ ERROR ]  Or because input shapes are incorrect (embedded to the model or passed via --input_shape).\n",
      "[ ERROR ]  Run Model Optimizer with --log_level=DEBUG for more information.\n",
      "[ ERROR ]  Exception occurred during running replacer \"REPLACEMENT_ID\" (<class 'extensions.middle.PartialInfer.PartialInfer'>): Stopped shape/value propagation at \"Einsum_56\" node. \n",
      " For more information please refer to Model Optimizer FAQ, question #38. (https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html?question=38#question-38)\n"
     ]
    }
   ],
   "source": [
    "# Converting from onnx to openvino fails because\n",
    "# openvino doesn't implements a operator named Einsum.\n",
    "!python3 /opt/intel/openvino_2021.2.200/deployment_tools/model_optimizer/mo.py --input_model xlnet-base-cased.onnx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino",
   "language": "python",
   "name": "openvino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
