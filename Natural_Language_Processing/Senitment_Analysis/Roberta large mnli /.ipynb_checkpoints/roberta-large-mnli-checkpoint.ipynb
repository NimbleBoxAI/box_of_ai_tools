{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "double-charleston",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters:  355362819\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch \n",
    "\n",
    "# Importing the tokenizer and model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\")\n",
    "\n",
    "# Model parameters.\n",
    "param = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total Parameters: \", param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "valuable-habitat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_ids: tensor([[   0,  100,  101,   47,    4, 1437,    2,    2,   38,  657,   47,    4,\n",
      "            2]])\n",
      "Attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Creating the inputs for the model with the help of the tokenizer. \n",
    "input_ids = tokenizer(\"I like you. </s></s> I love you.\", add_special_tokens=True, return_tensors=\"pt\").input_ids\n",
    "attn_mask = tokenizer(\"I like you. </s></s> I love you.\", add_special_tokens=True, return_tensors=\"pt\").attention_mask\n",
    "\n",
    "print(\"Input_ids:\", input_ids)\n",
    "print(\"Attention_mask\", attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "entire-relation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[6;30;42mPyTorch output:\u001b[0m SequenceClassifierOutput(loss=None, logits=tensor([[-0.1194,  0.4705, -0.4956]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Storing the pytorch model's output in pt_outputs variable.\n",
    "pt_outputs = model(input_ids, attn_mask)\n",
    "\n",
    "print('\\x1b[6;30;42m' + 'PyTorch output:' + '\\x1b[0m', pt_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "major-interstate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aakash/anaconda3/envs/openvino/lib/python3.7/site-packages/transformers/modeling_utils.py:1760: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n"
     ]
    }
   ],
   "source": [
    "# Converting the model to ONNX. \n",
    "torch.onnx.export(model,\n",
    "                 (input_ids, attn_mask),\n",
    "                 \"roberta-large-mnli.onnx\",\n",
    "                 input_names=[\"input_ids\", \"attn_mask\"],\n",
    "                 output_names=[\"outputs\"],\n",
    "                 opset_version=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "corrected-advertiser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/aakash/box_of_ai_tools/Natural_Language_Processing/Senitment_Analysis/Roberta large mnli /roberta-large-mnli.onnx\n",
      "\t- Path for generated IR: \t/home/aakash/box_of_ai_tools/Natural_Language_Processing/Senitment_Analysis/Roberta large mnli /.\n",
      "\t- IR output name: \troberta-large-mnli\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "ONNX specific parameters:\n",
      "Model Optimizer version: \t2021.2.0-1877-176bdf51370-releases/2021/2\n",
      "[ WARNING ]  Convert data type of Parameter \"input_ids\" to int32\n",
      "[ WARNING ]  Convert data type of Parameter \"attn_mask\" to int32\n",
      "\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/aakash/box_of_ai_tools/Natural_Language_Processing/Senitment_Analysis/Roberta large mnli /./roberta-large-mnli.xml\n",
      "[ SUCCESS ] BIN file: /home/aakash/box_of_ai_tools/Natural_Language_Processing/Senitment_Analysis/Roberta large mnli /./roberta-large-mnli.bin\n",
      "[ SUCCESS ] Total execution time: 137.22 seconds. \n",
      "[ SUCCESS ] Memory consumed: 4252 MB. \n"
     ]
    }
   ],
   "source": [
    "# Converting from onnx to openvino.\n",
    "!python3 /opt/intel/openvino_2021.2.200/deployment_tools/model_optimizer/mo.py --input_model roberta-large-mnli.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "involved-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore\n",
    "ie = IECore()\n",
    "\n",
    "openvino_mbart = ie.read_network(model=\"roberta-large-mnli.xml\", weights=\"roberta-large-mnli.bin\")\n",
    "exec_mbart = ie.load_network(network=openvino_mbart, device_name=\"CPU\", num_requests=1)\n",
    "openvino_outputs = exec_mbart.infer(inputs={\"input_ids\": input_ids,\n",
    "                                            \"attn_mask\": attn_mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "outside-motel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[6;30;42mOpenvino output:\u001b[0m {'outputs': array([[-0.11887613,  0.47044116, -0.49624145]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print('\\x1b[6;30;42m' + 'Openvino output:' + '\\x1b[0m', openvino_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino",
   "language": "python",
   "name": "openvino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
